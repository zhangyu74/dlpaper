{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe483034-3982-4427-b041-58e16718e446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 9.91M/9.91M [00:12<00:00, 774kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 28.9k/28.9k [00:00<00:00, 139kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1.65M/1.65M [00:08<00:00, 201kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Failed to download (trying next):\n",
      "HTTP Error 404: Not Found\n",
      "\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 4.54k/4.54k [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./datasets\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./datasets\\MNIST\\raw\n",
      "\n",
      "Examples in training set: 60000\n",
      "Examples in test set: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 会在代码同一级的目录下载训练数据，名称datasets，64M\n",
    "# n_epochs = 10运行时间约5分钟，为了减少时间改为n_epochs = 2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "transform = ToTensor()\n",
    "train_set = MNIST(root='./datasets', train=True, download=True, transform=transform)\n",
    "test_set = MNIST(root='./datasets', train=False, download=True, transform=transform)\n",
    "train_dataloader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_dataloader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "# dataset\n",
    "print(\"Examples in training set: {}\".format(len(train_dataloader.dataset)))\n",
    "print(\"Examples in test set: {}\".format(len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6495a8cb-91cb-4b26-9af8-51f130c0fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape:  torch.Size([1, 1, 28, 28])\n",
      "Patches shape:  torch.Size([1, 49, 8])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import Tensor\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels = 1, patch_size = 4, emb_size = 8):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            # break-down the image in s1 x s2 patches and flat them\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=patch_size, p2=patch_size),\n",
    "            nn.Linear(patch_size * patch_size * in_channels, emb_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.projection(x)\n",
    "        return x\n",
    "\n",
    "# Run a quick test\n",
    "sample_datapoint = torch.unsqueeze(train_set[0][0], 0)\n",
    "print(\"Initial shape: \", sample_datapoint.shape)\n",
    "embedding = PatchEmbedding()(sample_datapoint)\n",
    "print(\"Patches shape: \", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef58e60-1b1b-4277-8591-209fd60cba01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from einops import rearrange\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.att = torch.nn.MultiheadAttention(embed_dim=dim,\n",
    "                                               num_heads=n_heads,\n",
    "                                               dropout=dropout,\n",
    "                                               batch_first=True)\n",
    "        #self.q = torch.nn.Linear(dim, dim)\n",
    "        #self.k = torch.nn.Linear(dim, dim)\n",
    "        #self.v = torch.nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #q = self.q(x)\n",
    "        #k = self.k(x)\n",
    "        #v = self.v(x)\n",
    "        attn_output, attn_output_weights = self.att(x, x, x)\n",
    "        return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66808626-c30d-4896-afc5-3272619e84e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Attention(nn.Module):\\n    def __init__(self, dim, n_heads, dropout):\\n        super().__init__()\\n        self.d = dim\\n        self.n_heads = n_heads\\n\\n        assert self.d % n_heads == 0, f\"Can\\'t divide dimension {d} into {n_heads} heads\"\\n\\n        d_head = int(self.d / n_heads)\\n        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\\n        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\\n        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\\n        self.d_head = d_head\\n        self.softmax = nn.Softmax(dim=-1)\\n\\n    def forward(self, sequences):\\n        # Sequences has shape (N, seq_length, token_dim)\\n        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\\n        # And come back to    (N, seq_length, item_dim)  (through concatenation)\\n        result = []\\n        for sequence in sequences:\\n            seq_result = []\\n            for head in range(self.n_heads):\\n                q_mapping = self.q_mappings[head]\\n                k_mapping = self.k_mappings[head]\\n                v_mapping = self.v_mappings[head]\\n\\n                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\\n                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\\n\\n                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\\n                seq_result.append(attention @ v)\\n            result.append(torch.hstack(seq_result))\\n        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d = dim\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        assert self.d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "        d_head = int(self.d / n_heads)\n",
    "        self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "        self.d_head = d_head\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, sequences):\n",
    "        # Sequences has shape (N, seq_length, token_dim)\n",
    "        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "        result = []\n",
    "        for sequence in sequences:\n",
    "            seq_result = []\n",
    "            for head in range(self.n_heads):\n",
    "                q_mapping = self.q_mappings[head]\n",
    "                k_mapping = self.k_mappings[head]\n",
    "                v_mapping = self.v_mappings[head]\n",
    "\n",
    "                seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "                attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "                seq_result.append(attention @ v)\n",
    "            result.append(torch.hstack(seq_result))\n",
    "        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "486f5af5-f49a-4627-a1ec-71719fea20dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 49, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Attention(dim=8, n_heads=2, dropout=0.1)(torch.ones((1, 49, 8))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bf0bb9d-21df-4476-8786-02ab7b5ad623",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "719a0842-41e9-42cb-b941-4995cd4c0170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1297, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FeedForward(nn.Sequential):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "ff = FeedForward(dim=32, hidden_dim=32)\n",
    "ff(torch.ones((1, 1297, 32))).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e572819-2982-4353-8f09-e98d99750195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8022e2f-c42b-47a8-8883-c3af42f38ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import repeat\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self, ch=1, img_size=28, patch_size=4, emb_dim=8,\n",
    "                n_layers=2, out_dim=10, dropout=0., heads=2):\n",
    "        super(ViT, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.channels = ch\n",
    "        self.height = img_size\n",
    "        self.width = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Patching\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=ch,\n",
    "                                              patch_size=patch_size,\n",
    "                                              emb_size=emb_dim)\n",
    "        # Learnable params\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_patches + 1, emb_dim))\n",
    "        self.cls_token = nn.Parameter(torch.rand(1, 1, emb_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(n_layers):\n",
    "            transformer_block = nn.Sequential(\n",
    "                ResidualAdd(PreNorm(emb_dim, Attention(emb_dim, n_heads = heads, dropout = dropout))),\n",
    "                ResidualAdd(PreNorm(emb_dim, FeedForward(emb_dim, emb_dim, dropout = dropout))))\n",
    "            self.layers.append(transformer_block)\n",
    "\n",
    "        # Classification head\n",
    "        self.head = nn.Sequential(nn.LayerNorm(emb_dim), nn.Linear(emb_dim, out_dim))\n",
    "        \n",
    "        #self.head_ln = nn.LayerNorm(emb_dim)\n",
    "        #self.head = nn.Sequential(nn.Linear(int((1 + self.height/self.patch_size * self.width/self.patch_size) * emb_dim), out_dim))\n",
    "        \n",
    "        #self.head_ln = nn.LayerNorm(emb_dim)\n",
    "        #self.head = nn.Sequential(nn.Linear(emb_dim, out_dim), nn.Softmax(dim=-1))\n",
    "        \n",
    "        \n",
    "    def forward(self, img):\n",
    "        # Get patch embedding vectors\n",
    "        x = self.patch_embedding(img)\n",
    "        b, n, _ = x.shape\n",
    "\n",
    "        # Add cls token to inputs\n",
    "        cls_tokens = repeat(self.cls_token, '1 1 d -> b 1 d', b = b)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x += self.pos_embedding[:, :(n + 1)]\n",
    "\n",
    "        # Transformer layers\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers[i](x)\n",
    "\n",
    "        # Output based on classification token\n",
    "        return self.head(x[:, 0, :])\n",
    "\n",
    "        #x = x.view(x.shape[0], int((1 + self.height/self.patch_size * self.width/self.patch_size) * x.shape[-1]))\n",
    "        #out = self.head(x)\n",
    "        #return out\n",
    "        \n",
    "        #out = self.head(x[:, 0])\n",
    "        #return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a1979c-64b9-4c0d-9b2e-81e576c9c47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda (NVIDIA GeForce RTX 3070 Ti)\n",
      "Shape of example: torch.Size([128, 1, 28, 28])\n",
      "Shape of Patches: torch.Size([128, 49, 8])\n",
      "Shape of output: torch.Size([128, 10])\n"
     ]
    }
   ],
   "source": [
    "# Defining model and training options\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "model = ViT(ch=1, img_size=28, patch_size=4, emb_dim=8, n_layers=2, out_dim=10, dropout=0., heads=2).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_epochs = 10\n",
    "\n",
    "# torch\n",
    "inputs, labels = next(iter(train_dataloader))\n",
    "inputs, labels = inputs.to(device), labels.to(device)\n",
    "outputs = model(inputs)\n",
    "patches = model.patch_embedding(inputs)\n",
    "print(\"Shape of example: {}\".format(inputs.shape))\n",
    "print(\"Shape of Patches: {}\".format(patches.shape))\n",
    "print(\"Shape of output: {}\".format(outputs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e87fff64-25ea-4198-ad10-78bdcea83d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main training routine\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for step, (inputs, labels) in enumerate(train_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Store results\n",
    "        if step % 8 == 0:\n",
    "            print('Train Epoch: {} [{}/{}]\\tLoss: {:.6f}'.format(\n",
    "                epoch, step * len(inputs), len(train_dataloader.dataset), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "997858a9-148f-40f8-bdff-4060173c9ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on test data\n",
    "def test():\n",
    "    epoch_losses = []\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # Something was strange when using this?\n",
    "    # model.eval()\n",
    "    for step, (inputs, labels) in enumerate(test_dataloader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        epoch_losses.append(loss.item())\n",
    "        pred = outputs.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(labels.data.view_as(pred)).sum()\n",
    "    test_loss = np.mean(epoch_losses)\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "802fd2c1-a7d4-4396-af26-29c50dd9bd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000]\tLoss: 2.527013\n",
      "Train Epoch: 1 [1024/60000]\tLoss: 2.433336\n",
      "Train Epoch: 1 [2048/60000]\tLoss: 2.416459\n",
      "Train Epoch: 1 [3072/60000]\tLoss: 2.371042\n",
      "Train Epoch: 1 [4096/60000]\tLoss: 2.337822\n",
      "Train Epoch: 1 [5120/60000]\tLoss: 2.316396\n",
      "Train Epoch: 1 [6144/60000]\tLoss: 2.320098\n",
      "Train Epoch: 1 [7168/60000]\tLoss: 2.298880\n",
      "Train Epoch: 1 [8192/60000]\tLoss: 2.306011\n",
      "Train Epoch: 1 [9216/60000]\tLoss: 2.264112\n",
      "Train Epoch: 1 [10240/60000]\tLoss: 2.272666\n",
      "Train Epoch: 1 [11264/60000]\tLoss: 2.257117\n",
      "Train Epoch: 1 [12288/60000]\tLoss: 2.218036\n",
      "Train Epoch: 1 [13312/60000]\tLoss: 2.214975\n",
      "Train Epoch: 1 [14336/60000]\tLoss: 2.161872\n",
      "Train Epoch: 1 [15360/60000]\tLoss: 2.066006\n",
      "Train Epoch: 1 [16384/60000]\tLoss: 2.037601\n",
      "Train Epoch: 1 [17408/60000]\tLoss: 2.061960\n",
      "Train Epoch: 1 [18432/60000]\tLoss: 2.036791\n",
      "Train Epoch: 1 [19456/60000]\tLoss: 1.973611\n",
      "Train Epoch: 1 [20480/60000]\tLoss: 1.958320\n",
      "Train Epoch: 1 [21504/60000]\tLoss: 1.926242\n",
      "Train Epoch: 1 [22528/60000]\tLoss: 1.861042\n",
      "Train Epoch: 1 [23552/60000]\tLoss: 1.820456\n",
      "Train Epoch: 1 [24576/60000]\tLoss: 1.847583\n",
      "Train Epoch: 1 [25600/60000]\tLoss: 1.842925\n",
      "Train Epoch: 1 [26624/60000]\tLoss: 1.868645\n",
      "Train Epoch: 1 [27648/60000]\tLoss: 1.891280\n",
      "Train Epoch: 1 [28672/60000]\tLoss: 1.644579\n",
      "Train Epoch: 1 [29696/60000]\tLoss: 1.791618\n",
      "Train Epoch: 1 [30720/60000]\tLoss: 1.692804\n",
      "Train Epoch: 1 [31744/60000]\tLoss: 1.626775\n",
      "Train Epoch: 1 [32768/60000]\tLoss: 1.680117\n",
      "Train Epoch: 1 [33792/60000]\tLoss: 1.705429\n",
      "Train Epoch: 1 [34816/60000]\tLoss: 1.611905\n",
      "Train Epoch: 1 [35840/60000]\tLoss: 1.616782\n",
      "Train Epoch: 1 [36864/60000]\tLoss: 1.656623\n",
      "Train Epoch: 1 [37888/60000]\tLoss: 1.593571\n",
      "Train Epoch: 1 [38912/60000]\tLoss: 1.543424\n",
      "Train Epoch: 1 [39936/60000]\tLoss: 1.630628\n",
      "Train Epoch: 1 [40960/60000]\tLoss: 1.517482\n",
      "Train Epoch: 1 [41984/60000]\tLoss: 1.494528\n",
      "Train Epoch: 1 [43008/60000]\tLoss: 1.555495\n",
      "Train Epoch: 1 [44032/60000]\tLoss: 1.396413\n",
      "Train Epoch: 1 [45056/60000]\tLoss: 1.398653\n",
      "Train Epoch: 1 [46080/60000]\tLoss: 1.327629\n",
      "Train Epoch: 1 [47104/60000]\tLoss: 1.330512\n",
      "Train Epoch: 1 [48128/60000]\tLoss: 1.258893\n",
      "Train Epoch: 1 [49152/60000]\tLoss: 1.311749\n",
      "Train Epoch: 1 [50176/60000]\tLoss: 1.437163\n",
      "Train Epoch: 1 [51200/60000]\tLoss: 1.347856\n",
      "Train Epoch: 1 [52224/60000]\tLoss: 1.356946\n",
      "Train Epoch: 1 [53248/60000]\tLoss: 1.306224\n",
      "Train Epoch: 1 [54272/60000]\tLoss: 1.251959\n",
      "Train Epoch: 1 [55296/60000]\tLoss: 1.239947\n",
      "Train Epoch: 1 [56320/60000]\tLoss: 1.193845\n",
      "Train Epoch: 1 [57344/60000]\tLoss: 1.209251\n",
      "Train Epoch: 1 [58368/60000]\tLoss: 1.249182\n",
      "Train Epoch: 1 [59392/60000]\tLoss: 1.123148\n",
      "\n",
      "Test set: Avg. loss: 1.2223, Accuracy: 6100/10000 (61%)\n",
      "\n",
      "Train Epoch: 2 [0/60000]\tLoss: 1.226593\n",
      "Train Epoch: 2 [1024/60000]\tLoss: 1.281781\n",
      "Train Epoch: 2 [2048/60000]\tLoss: 1.414928\n",
      "Train Epoch: 2 [3072/60000]\tLoss: 1.208993\n",
      "Train Epoch: 2 [4096/60000]\tLoss: 1.012803\n",
      "Train Epoch: 2 [5120/60000]\tLoss: 1.164967\n",
      "Train Epoch: 2 [6144/60000]\tLoss: 1.306801\n",
      "Train Epoch: 2 [7168/60000]\tLoss: 1.176582\n",
      "Train Epoch: 2 [8192/60000]\tLoss: 1.175919\n",
      "Train Epoch: 2 [9216/60000]\tLoss: 1.245808\n",
      "Train Epoch: 2 [10240/60000]\tLoss: 1.104084\n",
      "Train Epoch: 2 [11264/60000]\tLoss: 1.043905\n",
      "Train Epoch: 2 [12288/60000]\tLoss: 1.023214\n",
      "Train Epoch: 2 [13312/60000]\tLoss: 1.048848\n",
      "Train Epoch: 2 [14336/60000]\tLoss: 1.160187\n",
      "Train Epoch: 2 [15360/60000]\tLoss: 1.166639\n",
      "Train Epoch: 2 [16384/60000]\tLoss: 1.028823\n",
      "Train Epoch: 2 [17408/60000]\tLoss: 1.019381\n",
      "Train Epoch: 2 [18432/60000]\tLoss: 1.237050\n",
      "Train Epoch: 2 [19456/60000]\tLoss: 1.042023\n",
      "Train Epoch: 2 [20480/60000]\tLoss: 1.242332\n",
      "Train Epoch: 2 [21504/60000]\tLoss: 0.889042\n",
      "Train Epoch: 2 [22528/60000]\tLoss: 0.913031\n",
      "Train Epoch: 2 [23552/60000]\tLoss: 1.197056\n",
      "Train Epoch: 2 [24576/60000]\tLoss: 0.922062\n",
      "Train Epoch: 2 [25600/60000]\tLoss: 1.024689\n",
      "Train Epoch: 2 [26624/60000]\tLoss: 0.844145\n",
      "Train Epoch: 2 [27648/60000]\tLoss: 0.983103\n",
      "Train Epoch: 2 [28672/60000]\tLoss: 1.093685\n",
      "Train Epoch: 2 [29696/60000]\tLoss: 0.965443\n",
      "Train Epoch: 2 [30720/60000]\tLoss: 1.049847\n",
      "Train Epoch: 2 [31744/60000]\tLoss: 1.009396\n",
      "Train Epoch: 2 [32768/60000]\tLoss: 1.083855\n",
      "Train Epoch: 2 [33792/60000]\tLoss: 1.117492\n",
      "Train Epoch: 2 [34816/60000]\tLoss: 1.074193\n",
      "Train Epoch: 2 [35840/60000]\tLoss: 0.890986\n",
      "Train Epoch: 2 [36864/60000]\tLoss: 1.072267\n",
      "Train Epoch: 2 [37888/60000]\tLoss: 0.881659\n",
      "Train Epoch: 2 [38912/60000]\tLoss: 1.044238\n",
      "Train Epoch: 2 [39936/60000]\tLoss: 1.041446\n",
      "Train Epoch: 2 [40960/60000]\tLoss: 1.074763\n",
      "Train Epoch: 2 [41984/60000]\tLoss: 0.967376\n",
      "Train Epoch: 2 [43008/60000]\tLoss: 1.095573\n",
      "Train Epoch: 2 [44032/60000]\tLoss: 0.885650\n",
      "Train Epoch: 2 [45056/60000]\tLoss: 0.945818\n",
      "Train Epoch: 2 [46080/60000]\tLoss: 1.019547\n",
      "Train Epoch: 2 [47104/60000]\tLoss: 0.816398\n",
      "Train Epoch: 2 [48128/60000]\tLoss: 0.989450\n",
      "Train Epoch: 2 [49152/60000]\tLoss: 0.812286\n",
      "Train Epoch: 2 [50176/60000]\tLoss: 0.977761\n",
      "Train Epoch: 2 [51200/60000]\tLoss: 0.979865\n",
      "Train Epoch: 2 [52224/60000]\tLoss: 0.947519\n",
      "Train Epoch: 2 [53248/60000]\tLoss: 0.854334\n",
      "Train Epoch: 2 [54272/60000]\tLoss: 1.011385\n",
      "Train Epoch: 2 [55296/60000]\tLoss: 0.958941\n",
      "Train Epoch: 2 [56320/60000]\tLoss: 1.035468\n",
      "Train Epoch: 2 [57344/60000]\tLoss: 0.996164\n",
      "Train Epoch: 2 [58368/60000]\tLoss: 0.811838\n",
      "Train Epoch: 2 [59392/60000]\tLoss: 0.871335\n",
      "\n",
      "Test set: Avg. loss: 0.8566, Accuracy: 7258/10000 (73%)\n",
      "\n",
      "Train Epoch: 3 [0/60000]\tLoss: 0.924084\n",
      "Train Epoch: 3 [1024/60000]\tLoss: 0.882874\n",
      "Train Epoch: 3 [2048/60000]\tLoss: 0.914548\n",
      "Train Epoch: 3 [3072/60000]\tLoss: 0.791885\n",
      "Train Epoch: 3 [4096/60000]\tLoss: 0.914421\n",
      "Train Epoch: 3 [5120/60000]\tLoss: 0.950397\n",
      "Train Epoch: 3 [6144/60000]\tLoss: 0.814627\n",
      "Train Epoch: 3 [7168/60000]\tLoss: 0.899545\n",
      "Train Epoch: 3 [8192/60000]\tLoss: 0.785583\n",
      "Train Epoch: 3 [9216/60000]\tLoss: 0.874088\n",
      "Train Epoch: 3 [10240/60000]\tLoss: 0.833642\n",
      "Train Epoch: 3 [11264/60000]\tLoss: 0.908871\n",
      "Train Epoch: 3 [12288/60000]\tLoss: 0.882027\n",
      "Train Epoch: 3 [13312/60000]\tLoss: 0.862135\n",
      "Train Epoch: 3 [14336/60000]\tLoss: 0.946408\n",
      "Train Epoch: 3 [15360/60000]\tLoss: 0.727750\n",
      "Train Epoch: 3 [16384/60000]\tLoss: 0.884478\n",
      "Train Epoch: 3 [17408/60000]\tLoss: 0.838051\n",
      "Train Epoch: 3 [18432/60000]\tLoss: 0.866617\n",
      "Train Epoch: 3 [19456/60000]\tLoss: 0.677008\n",
      "Train Epoch: 3 [20480/60000]\tLoss: 0.785644\n",
      "Train Epoch: 3 [21504/60000]\tLoss: 0.995055\n",
      "Train Epoch: 3 [22528/60000]\tLoss: 0.802127\n",
      "Train Epoch: 3 [23552/60000]\tLoss: 0.920683\n",
      "Train Epoch: 3 [24576/60000]\tLoss: 0.633889\n",
      "Train Epoch: 3 [25600/60000]\tLoss: 0.707505\n",
      "Train Epoch: 3 [26624/60000]\tLoss: 0.919050\n",
      "Train Epoch: 3 [27648/60000]\tLoss: 0.895663\n",
      "Train Epoch: 3 [28672/60000]\tLoss: 0.723525\n",
      "Train Epoch: 3 [29696/60000]\tLoss: 0.789363\n",
      "Train Epoch: 3 [30720/60000]\tLoss: 0.776514\n",
      "Train Epoch: 3 [31744/60000]\tLoss: 0.765816\n",
      "Train Epoch: 3 [32768/60000]\tLoss: 0.759475\n",
      "Train Epoch: 3 [33792/60000]\tLoss: 0.975128\n",
      "Train Epoch: 3 [34816/60000]\tLoss: 0.789181\n",
      "Train Epoch: 3 [35840/60000]\tLoss: 0.712885\n",
      "Train Epoch: 3 [36864/60000]\tLoss: 0.796830\n",
      "Train Epoch: 3 [37888/60000]\tLoss: 0.786319\n",
      "Train Epoch: 3 [38912/60000]\tLoss: 0.814843\n",
      "Train Epoch: 3 [39936/60000]\tLoss: 0.861540\n",
      "Train Epoch: 3 [40960/60000]\tLoss: 0.739769\n",
      "Train Epoch: 3 [41984/60000]\tLoss: 0.679882\n",
      "Train Epoch: 3 [43008/60000]\tLoss: 0.864045\n",
      "Train Epoch: 3 [44032/60000]\tLoss: 0.624946\n",
      "Train Epoch: 3 [45056/60000]\tLoss: 0.685731\n",
      "Train Epoch: 3 [46080/60000]\tLoss: 0.638376\n",
      "Train Epoch: 3 [47104/60000]\tLoss: 0.757531\n",
      "Train Epoch: 3 [48128/60000]\tLoss: 0.920382\n",
      "Train Epoch: 3 [49152/60000]\tLoss: 0.761980\n",
      "Train Epoch: 3 [50176/60000]\tLoss: 0.699522\n",
      "Train Epoch: 3 [51200/60000]\tLoss: 0.668252\n",
      "Train Epoch: 3 [52224/60000]\tLoss: 0.502722\n",
      "Train Epoch: 3 [53248/60000]\tLoss: 0.686157\n",
      "Train Epoch: 3 [54272/60000]\tLoss: 0.726044\n",
      "Train Epoch: 3 [55296/60000]\tLoss: 0.766138\n",
      "Train Epoch: 3 [56320/60000]\tLoss: 0.953501\n",
      "Train Epoch: 3 [57344/60000]\tLoss: 0.669592\n",
      "Train Epoch: 3 [58368/60000]\tLoss: 0.756146\n",
      "Train Epoch: 3 [59392/60000]\tLoss: 0.789550\n",
      "\n",
      "Test set: Avg. loss: 0.6978, Accuracy: 7730/10000 (77%)\n",
      "\n",
      "Train Epoch: 4 [0/60000]\tLoss: 0.912243\n",
      "Train Epoch: 4 [1024/60000]\tLoss: 0.748551\n",
      "Train Epoch: 4 [2048/60000]\tLoss: 0.614578\n",
      "Train Epoch: 4 [3072/60000]\tLoss: 0.702852\n",
      "Train Epoch: 4 [4096/60000]\tLoss: 0.757644\n",
      "Train Epoch: 4 [5120/60000]\tLoss: 0.707494\n",
      "Train Epoch: 4 [6144/60000]\tLoss: 0.702051\n",
      "Train Epoch: 4 [7168/60000]\tLoss: 0.917597\n",
      "Train Epoch: 4 [8192/60000]\tLoss: 0.660976\n",
      "Train Epoch: 4 [9216/60000]\tLoss: 0.669818\n",
      "Train Epoch: 4 [10240/60000]\tLoss: 0.701184\n",
      "Train Epoch: 4 [11264/60000]\tLoss: 0.905011\n",
      "Train Epoch: 4 [12288/60000]\tLoss: 0.688668\n",
      "Train Epoch: 4 [13312/60000]\tLoss: 0.765562\n",
      "Train Epoch: 4 [14336/60000]\tLoss: 0.813714\n",
      "Train Epoch: 4 [15360/60000]\tLoss: 0.728113\n",
      "Train Epoch: 4 [16384/60000]\tLoss: 0.800962\n",
      "Train Epoch: 4 [17408/60000]\tLoss: 0.684303\n",
      "Train Epoch: 4 [18432/60000]\tLoss: 0.749393\n",
      "Train Epoch: 4 [19456/60000]\tLoss: 0.816197\n",
      "Train Epoch: 4 [20480/60000]\tLoss: 0.613098\n",
      "Train Epoch: 4 [21504/60000]\tLoss: 0.482941\n",
      "Train Epoch: 4 [22528/60000]\tLoss: 0.753252\n",
      "Train Epoch: 4 [23552/60000]\tLoss: 0.523233\n",
      "Train Epoch: 4 [24576/60000]\tLoss: 0.740824\n",
      "Train Epoch: 4 [25600/60000]\tLoss: 0.567486\n",
      "Train Epoch: 4 [26624/60000]\tLoss: 0.760352\n",
      "Train Epoch: 4 [27648/60000]\tLoss: 0.556709\n",
      "Train Epoch: 4 [28672/60000]\tLoss: 0.579992\n",
      "Train Epoch: 4 [29696/60000]\tLoss: 0.643328\n",
      "Train Epoch: 4 [30720/60000]\tLoss: 0.589434\n",
      "Train Epoch: 4 [31744/60000]\tLoss: 0.740961\n",
      "Train Epoch: 4 [32768/60000]\tLoss: 0.697979\n",
      "Train Epoch: 4 [33792/60000]\tLoss: 0.669222\n",
      "Train Epoch: 4 [34816/60000]\tLoss: 0.535947\n",
      "Train Epoch: 4 [35840/60000]\tLoss: 0.822677\n",
      "Train Epoch: 4 [36864/60000]\tLoss: 0.733012\n",
      "Train Epoch: 4 [37888/60000]\tLoss: 0.594064\n",
      "Train Epoch: 4 [38912/60000]\tLoss: 0.616182\n",
      "Train Epoch: 4 [39936/60000]\tLoss: 0.794205\n",
      "Train Epoch: 4 [40960/60000]\tLoss: 0.661435\n",
      "Train Epoch: 4 [41984/60000]\tLoss: 0.812369\n",
      "Train Epoch: 4 [43008/60000]\tLoss: 0.686269\n",
      "Train Epoch: 4 [44032/60000]\tLoss: 0.629766\n",
      "Train Epoch: 4 [45056/60000]\tLoss: 0.550391\n",
      "Train Epoch: 4 [46080/60000]\tLoss: 0.648196\n",
      "Train Epoch: 4 [47104/60000]\tLoss: 0.618726\n",
      "Train Epoch: 4 [48128/60000]\tLoss: 0.648289\n",
      "Train Epoch: 4 [49152/60000]\tLoss: 0.743044\n",
      "Train Epoch: 4 [50176/60000]\tLoss: 0.653921\n",
      "Train Epoch: 4 [51200/60000]\tLoss: 0.594488\n",
      "Train Epoch: 4 [52224/60000]\tLoss: 0.739638\n",
      "Train Epoch: 4 [53248/60000]\tLoss: 0.521627\n",
      "Train Epoch: 4 [54272/60000]\tLoss: 0.692747\n",
      "Train Epoch: 4 [55296/60000]\tLoss: 0.856984\n",
      "Train Epoch: 4 [56320/60000]\tLoss: 0.483147\n",
      "Train Epoch: 4 [57344/60000]\tLoss: 0.771821\n",
      "Train Epoch: 4 [58368/60000]\tLoss: 0.619834\n",
      "Train Epoch: 4 [59392/60000]\tLoss: 0.762281\n",
      "\n",
      "Test set: Avg. loss: 0.6282, Accuracy: 8017/10000 (80%)\n",
      "\n",
      "Train Epoch: 5 [0/60000]\tLoss: 0.658040\n",
      "Train Epoch: 5 [1024/60000]\tLoss: 0.774942\n",
      "Train Epoch: 5 [2048/60000]\tLoss: 0.698891\n",
      "Train Epoch: 5 [3072/60000]\tLoss: 0.498977\n",
      "Train Epoch: 5 [4096/60000]\tLoss: 0.578070\n",
      "Train Epoch: 5 [5120/60000]\tLoss: 0.599221\n",
      "Train Epoch: 5 [6144/60000]\tLoss: 0.554343\n",
      "Train Epoch: 5 [7168/60000]\tLoss: 0.647946\n",
      "Train Epoch: 5 [8192/60000]\tLoss: 0.730254\n",
      "Train Epoch: 5 [9216/60000]\tLoss: 0.697031\n",
      "Train Epoch: 5 [10240/60000]\tLoss: 0.442061\n",
      "Train Epoch: 5 [11264/60000]\tLoss: 0.557925\n",
      "Train Epoch: 5 [12288/60000]\tLoss: 0.635354\n",
      "Train Epoch: 5 [13312/60000]\tLoss: 0.654393\n",
      "Train Epoch: 5 [14336/60000]\tLoss: 0.640358\n",
      "Train Epoch: 5 [15360/60000]\tLoss: 0.512971\n",
      "Train Epoch: 5 [16384/60000]\tLoss: 0.548662\n",
      "Train Epoch: 5 [17408/60000]\tLoss: 0.560223\n",
      "Train Epoch: 5 [18432/60000]\tLoss: 0.651398\n",
      "Train Epoch: 5 [19456/60000]\tLoss: 0.608895\n",
      "Train Epoch: 5 [20480/60000]\tLoss: 0.858439\n",
      "Train Epoch: 5 [21504/60000]\tLoss: 0.742840\n",
      "Train Epoch: 5 [22528/60000]\tLoss: 0.479256\n",
      "Train Epoch: 5 [23552/60000]\tLoss: 0.508156\n",
      "Train Epoch: 5 [24576/60000]\tLoss: 0.959185\n",
      "Train Epoch: 5 [25600/60000]\tLoss: 0.546905\n",
      "Train Epoch: 5 [26624/60000]\tLoss: 0.605676\n",
      "Train Epoch: 5 [27648/60000]\tLoss: 0.608151\n",
      "Train Epoch: 5 [28672/60000]\tLoss: 0.601526\n",
      "Train Epoch: 5 [29696/60000]\tLoss: 0.593443\n",
      "Train Epoch: 5 [30720/60000]\tLoss: 0.710965\n",
      "Train Epoch: 5 [31744/60000]\tLoss: 0.478047\n",
      "Train Epoch: 5 [32768/60000]\tLoss: 0.536675\n",
      "Train Epoch: 5 [33792/60000]\tLoss: 0.482847\n",
      "Train Epoch: 5 [34816/60000]\tLoss: 0.564906\n",
      "Train Epoch: 5 [35840/60000]\tLoss: 0.713000\n",
      "Train Epoch: 5 [36864/60000]\tLoss: 0.654525\n",
      "Train Epoch: 5 [37888/60000]\tLoss: 0.524443\n",
      "Train Epoch: 5 [38912/60000]\tLoss: 0.515541\n",
      "Train Epoch: 5 [39936/60000]\tLoss: 0.593441\n",
      "Train Epoch: 5 [40960/60000]\tLoss: 0.634901\n",
      "Train Epoch: 5 [41984/60000]\tLoss: 0.684904\n",
      "Train Epoch: 5 [43008/60000]\tLoss: 0.661647\n",
      "Train Epoch: 5 [44032/60000]\tLoss: 0.854152\n",
      "Train Epoch: 5 [45056/60000]\tLoss: 0.770084\n",
      "Train Epoch: 5 [46080/60000]\tLoss: 0.496023\n",
      "Train Epoch: 5 [47104/60000]\tLoss: 0.526502\n",
      "Train Epoch: 5 [48128/60000]\tLoss: 0.556956\n",
      "Train Epoch: 5 [49152/60000]\tLoss: 0.620820\n",
      "Train Epoch: 5 [50176/60000]\tLoss: 0.523354\n",
      "Train Epoch: 5 [51200/60000]\tLoss: 0.585920\n",
      "Train Epoch: 5 [52224/60000]\tLoss: 0.485243\n",
      "Train Epoch: 5 [53248/60000]\tLoss: 0.678942\n",
      "Train Epoch: 5 [54272/60000]\tLoss: 0.790683\n",
      "Train Epoch: 5 [55296/60000]\tLoss: 0.556781\n",
      "Train Epoch: 5 [56320/60000]\tLoss: 0.686165\n",
      "Train Epoch: 5 [57344/60000]\tLoss: 0.755034\n",
      "Train Epoch: 5 [58368/60000]\tLoss: 0.588780\n",
      "Train Epoch: 5 [59392/60000]\tLoss: 0.688992\n",
      "\n",
      "Test set: Avg. loss: 0.5607, Accuracy: 8220/10000 (82%)\n",
      "\n",
      "Train Epoch: 6 [0/60000]\tLoss: 0.535577\n",
      "Train Epoch: 6 [1024/60000]\tLoss: 0.595936\n",
      "Train Epoch: 6 [2048/60000]\tLoss: 0.572128\n",
      "Train Epoch: 6 [3072/60000]\tLoss: 0.552193\n",
      "Train Epoch: 6 [4096/60000]\tLoss: 0.754288\n",
      "Train Epoch: 6 [5120/60000]\tLoss: 0.578797\n",
      "Train Epoch: 6 [6144/60000]\tLoss: 0.620929\n",
      "Train Epoch: 6 [7168/60000]\tLoss: 0.616060\n",
      "Train Epoch: 6 [8192/60000]\tLoss: 0.519708\n",
      "Train Epoch: 6 [9216/60000]\tLoss: 0.358242\n",
      "Train Epoch: 6 [10240/60000]\tLoss: 0.517505\n",
      "Train Epoch: 6 [11264/60000]\tLoss: 0.444032\n",
      "Train Epoch: 6 [12288/60000]\tLoss: 0.576123\n",
      "Train Epoch: 6 [13312/60000]\tLoss: 0.577778\n",
      "Train Epoch: 6 [14336/60000]\tLoss: 0.412941\n",
      "Train Epoch: 6 [15360/60000]\tLoss: 0.484253\n",
      "Train Epoch: 6 [16384/60000]\tLoss: 0.691994\n",
      "Train Epoch: 6 [17408/60000]\tLoss: 0.611556\n",
      "Train Epoch: 6 [18432/60000]\tLoss: 0.524087\n",
      "Train Epoch: 6 [19456/60000]\tLoss: 0.540220\n",
      "Train Epoch: 6 [20480/60000]\tLoss: 0.553280\n",
      "Train Epoch: 6 [21504/60000]\tLoss: 0.559306\n",
      "Train Epoch: 6 [22528/60000]\tLoss: 0.493662\n",
      "Train Epoch: 6 [23552/60000]\tLoss: 0.631406\n",
      "Train Epoch: 6 [24576/60000]\tLoss: 0.511103\n",
      "Train Epoch: 6 [25600/60000]\tLoss: 0.532286\n",
      "Train Epoch: 6 [26624/60000]\tLoss: 0.627043\n",
      "Train Epoch: 6 [27648/60000]\tLoss: 0.422626\n",
      "Train Epoch: 6 [28672/60000]\tLoss: 0.433188\n",
      "Train Epoch: 6 [29696/60000]\tLoss: 0.490571\n",
      "Train Epoch: 6 [30720/60000]\tLoss: 0.544048\n",
      "Train Epoch: 6 [31744/60000]\tLoss: 0.417835\n",
      "Train Epoch: 6 [32768/60000]\tLoss: 0.593787\n",
      "Train Epoch: 6 [33792/60000]\tLoss: 0.490216\n",
      "Train Epoch: 6 [34816/60000]\tLoss: 0.521230\n",
      "Train Epoch: 6 [35840/60000]\tLoss: 0.592797\n",
      "Train Epoch: 6 [36864/60000]\tLoss: 0.580556\n",
      "Train Epoch: 6 [37888/60000]\tLoss: 0.484467\n",
      "Train Epoch: 6 [38912/60000]\tLoss: 0.423458\n",
      "Train Epoch: 6 [39936/60000]\tLoss: 0.490106\n",
      "Train Epoch: 6 [40960/60000]\tLoss: 0.478813\n",
      "Train Epoch: 6 [41984/60000]\tLoss: 0.492145\n",
      "Train Epoch: 6 [43008/60000]\tLoss: 0.467877\n",
      "Train Epoch: 6 [44032/60000]\tLoss: 0.694739\n",
      "Train Epoch: 6 [45056/60000]\tLoss: 0.661999\n",
      "Train Epoch: 6 [46080/60000]\tLoss: 0.598502\n",
      "Train Epoch: 6 [47104/60000]\tLoss: 0.507530\n",
      "Train Epoch: 6 [48128/60000]\tLoss: 0.567984\n",
      "Train Epoch: 6 [49152/60000]\tLoss: 0.562338\n",
      "Train Epoch: 6 [50176/60000]\tLoss: 0.570560\n",
      "Train Epoch: 6 [51200/60000]\tLoss: 0.526415\n",
      "Train Epoch: 6 [52224/60000]\tLoss: 0.518383\n",
      "Train Epoch: 6 [53248/60000]\tLoss: 0.675000\n",
      "Train Epoch: 6 [54272/60000]\tLoss: 0.854860\n",
      "Train Epoch: 6 [55296/60000]\tLoss: 0.686646\n",
      "Train Epoch: 6 [56320/60000]\tLoss: 0.567528\n",
      "Train Epoch: 6 [57344/60000]\tLoss: 0.676655\n",
      "Train Epoch: 6 [58368/60000]\tLoss: 0.392970\n",
      "Train Epoch: 6 [59392/60000]\tLoss: 0.516603\n",
      "\n",
      "Test set: Avg. loss: 0.5011, Accuracy: 8419/10000 (84%)\n",
      "\n",
      "Train Epoch: 7 [0/60000]\tLoss: 0.604904\n",
      "Train Epoch: 7 [1024/60000]\tLoss: 0.426642\n",
      "Train Epoch: 7 [2048/60000]\tLoss: 0.533420\n",
      "Train Epoch: 7 [3072/60000]\tLoss: 0.476117\n",
      "Train Epoch: 7 [4096/60000]\tLoss: 0.504702\n",
      "Train Epoch: 7 [5120/60000]\tLoss: 0.433546\n",
      "Train Epoch: 7 [6144/60000]\tLoss: 0.479376\n",
      "Train Epoch: 7 [7168/60000]\tLoss: 0.495492\n",
      "Train Epoch: 7 [8192/60000]\tLoss: 0.534106\n",
      "Train Epoch: 7 [9216/60000]\tLoss: 0.468820\n",
      "Train Epoch: 7 [10240/60000]\tLoss: 0.709221\n",
      "Train Epoch: 7 [11264/60000]\tLoss: 0.514320\n",
      "Train Epoch: 7 [12288/60000]\tLoss: 0.517915\n",
      "Train Epoch: 7 [13312/60000]\tLoss: 0.393207\n",
      "Train Epoch: 7 [14336/60000]\tLoss: 0.412175\n",
      "Train Epoch: 7 [15360/60000]\tLoss: 0.483300\n",
      "Train Epoch: 7 [16384/60000]\tLoss: 0.549233\n",
      "Train Epoch: 7 [17408/60000]\tLoss: 0.556143\n",
      "Train Epoch: 7 [18432/60000]\tLoss: 0.576758\n",
      "Train Epoch: 7 [19456/60000]\tLoss: 0.541891\n",
      "Train Epoch: 7 [20480/60000]\tLoss: 0.420910\n",
      "Train Epoch: 7 [21504/60000]\tLoss: 0.447456\n",
      "Train Epoch: 7 [22528/60000]\tLoss: 0.650055\n",
      "Train Epoch: 7 [23552/60000]\tLoss: 0.373264\n",
      "Train Epoch: 7 [24576/60000]\tLoss: 0.606331\n",
      "Train Epoch: 7 [25600/60000]\tLoss: 0.483585\n",
      "Train Epoch: 7 [26624/60000]\tLoss: 0.391498\n",
      "Train Epoch: 7 [27648/60000]\tLoss: 0.634996\n",
      "Train Epoch: 7 [28672/60000]\tLoss: 0.644067\n",
      "Train Epoch: 7 [29696/60000]\tLoss: 0.589231\n",
      "Train Epoch: 7 [30720/60000]\tLoss: 0.416644\n",
      "Train Epoch: 7 [31744/60000]\tLoss: 0.466289\n",
      "Train Epoch: 7 [32768/60000]\tLoss: 0.624360\n",
      "Train Epoch: 7 [33792/60000]\tLoss: 0.563394\n",
      "Train Epoch: 7 [34816/60000]\tLoss: 0.626408\n",
      "Train Epoch: 7 [35840/60000]\tLoss: 0.492115\n",
      "Train Epoch: 7 [36864/60000]\tLoss: 0.595327\n",
      "Train Epoch: 7 [37888/60000]\tLoss: 0.563395\n",
      "Train Epoch: 7 [38912/60000]\tLoss: 0.491750\n",
      "Train Epoch: 7 [39936/60000]\tLoss: 0.805482\n",
      "Train Epoch: 7 [40960/60000]\tLoss: 0.427266\n",
      "Train Epoch: 7 [41984/60000]\tLoss: 0.468355\n",
      "Train Epoch: 7 [43008/60000]\tLoss: 0.367771\n",
      "Train Epoch: 7 [44032/60000]\tLoss: 0.628097\n",
      "Train Epoch: 7 [45056/60000]\tLoss: 0.399497\n",
      "Train Epoch: 7 [46080/60000]\tLoss: 0.510631\n",
      "Train Epoch: 7 [47104/60000]\tLoss: 0.524593\n",
      "Train Epoch: 7 [48128/60000]\tLoss: 0.481368\n",
      "Train Epoch: 7 [49152/60000]\tLoss: 0.646741\n",
      "Train Epoch: 7 [50176/60000]\tLoss: 0.465769\n",
      "Train Epoch: 7 [51200/60000]\tLoss: 0.368257\n",
      "Train Epoch: 7 [52224/60000]\tLoss: 0.484635\n",
      "Train Epoch: 7 [53248/60000]\tLoss: 0.622509\n",
      "Train Epoch: 7 [54272/60000]\tLoss: 0.527443\n",
      "Train Epoch: 7 [55296/60000]\tLoss: 0.497015\n",
      "Train Epoch: 7 [56320/60000]\tLoss: 0.538195\n",
      "Train Epoch: 7 [57344/60000]\tLoss: 0.429345\n",
      "Train Epoch: 7 [58368/60000]\tLoss: 0.416139\n",
      "Train Epoch: 7 [59392/60000]\tLoss: 0.664460\n",
      "\n",
      "Test set: Avg. loss: 0.4800, Accuracy: 8495/10000 (85%)\n",
      "\n",
      "Train Epoch: 8 [0/60000]\tLoss: 0.794163\n",
      "Train Epoch: 8 [1024/60000]\tLoss: 0.507417\n",
      "Train Epoch: 8 [2048/60000]\tLoss: 0.547075\n",
      "Train Epoch: 8 [3072/60000]\tLoss: 0.586757\n",
      "Train Epoch: 8 [4096/60000]\tLoss: 0.453095\n",
      "Train Epoch: 8 [5120/60000]\tLoss: 0.623851\n",
      "Train Epoch: 8 [6144/60000]\tLoss: 0.594557\n",
      "Train Epoch: 8 [7168/60000]\tLoss: 0.593868\n",
      "Train Epoch: 8 [8192/60000]\tLoss: 0.401735\n",
      "Train Epoch: 8 [9216/60000]\tLoss: 0.512596\n",
      "Train Epoch: 8 [10240/60000]\tLoss: 0.538651\n",
      "Train Epoch: 8 [11264/60000]\tLoss: 0.678421\n",
      "Train Epoch: 8 [12288/60000]\tLoss: 0.419255\n",
      "Train Epoch: 8 [13312/60000]\tLoss: 0.590924\n",
      "Train Epoch: 8 [14336/60000]\tLoss: 0.518114\n",
      "Train Epoch: 8 [15360/60000]\tLoss: 0.498176\n",
      "Train Epoch: 8 [16384/60000]\tLoss: 0.488986\n",
      "Train Epoch: 8 [17408/60000]\tLoss: 0.338034\n",
      "Train Epoch: 8 [18432/60000]\tLoss: 0.600607\n",
      "Train Epoch: 8 [19456/60000]\tLoss: 0.510184\n",
      "Train Epoch: 8 [20480/60000]\tLoss: 0.638325\n",
      "Train Epoch: 8 [21504/60000]\tLoss: 0.357490\n",
      "Train Epoch: 8 [22528/60000]\tLoss: 0.452557\n",
      "Train Epoch: 8 [23552/60000]\tLoss: 0.492257\n",
      "Train Epoch: 8 [24576/60000]\tLoss: 0.468861\n",
      "Train Epoch: 8 [25600/60000]\tLoss: 0.548973\n",
      "Train Epoch: 8 [26624/60000]\tLoss: 0.551067\n",
      "Train Epoch: 8 [27648/60000]\tLoss: 0.491443\n",
      "Train Epoch: 8 [28672/60000]\tLoss: 0.586162\n",
      "Train Epoch: 8 [29696/60000]\tLoss: 0.487961\n",
      "Train Epoch: 8 [30720/60000]\tLoss: 0.544717\n",
      "Train Epoch: 8 [31744/60000]\tLoss: 0.382133\n",
      "Train Epoch: 8 [32768/60000]\tLoss: 0.560776\n",
      "Train Epoch: 8 [33792/60000]\tLoss: 0.387965\n",
      "Train Epoch: 8 [34816/60000]\tLoss: 0.383213\n",
      "Train Epoch: 8 [35840/60000]\tLoss: 0.499189\n",
      "Train Epoch: 8 [36864/60000]\tLoss: 0.573377\n",
      "Train Epoch: 8 [37888/60000]\tLoss: 0.489136\n",
      "Train Epoch: 8 [38912/60000]\tLoss: 0.499579\n",
      "Train Epoch: 8 [39936/60000]\tLoss: 0.451550\n",
      "Train Epoch: 8 [40960/60000]\tLoss: 0.479921\n",
      "Train Epoch: 8 [41984/60000]\tLoss: 0.421211\n",
      "Train Epoch: 8 [43008/60000]\tLoss: 0.353621\n",
      "Train Epoch: 8 [44032/60000]\tLoss: 0.630229\n",
      "Train Epoch: 8 [45056/60000]\tLoss: 0.537071\n",
      "Train Epoch: 8 [46080/60000]\tLoss: 0.629958\n",
      "Train Epoch: 8 [47104/60000]\tLoss: 0.569685\n",
      "Train Epoch: 8 [48128/60000]\tLoss: 0.520283\n",
      "Train Epoch: 8 [49152/60000]\tLoss: 0.455422\n",
      "Train Epoch: 8 [50176/60000]\tLoss: 0.506507\n",
      "Train Epoch: 8 [51200/60000]\tLoss: 0.494758\n",
      "Train Epoch: 8 [52224/60000]\tLoss: 0.412975\n",
      "Train Epoch: 8 [53248/60000]\tLoss: 0.490483\n",
      "Train Epoch: 8 [54272/60000]\tLoss: 0.540078\n",
      "Train Epoch: 8 [55296/60000]\tLoss: 0.341963\n",
      "Train Epoch: 8 [56320/60000]\tLoss: 0.469480\n",
      "Train Epoch: 8 [57344/60000]\tLoss: 0.383058\n",
      "Train Epoch: 8 [58368/60000]\tLoss: 0.440315\n",
      "Train Epoch: 8 [59392/60000]\tLoss: 0.364422\n",
      "\n",
      "Test set: Avg. loss: 0.4365, Accuracy: 8643/10000 (86%)\n",
      "\n",
      "Train Epoch: 9 [0/60000]\tLoss: 0.362362\n",
      "Train Epoch: 9 [1024/60000]\tLoss: 0.474931\n",
      "Train Epoch: 9 [2048/60000]\tLoss: 0.648263\n",
      "Train Epoch: 9 [3072/60000]\tLoss: 0.393125\n",
      "Train Epoch: 9 [4096/60000]\tLoss: 0.452363\n",
      "Train Epoch: 9 [5120/60000]\tLoss: 0.409848\n",
      "Train Epoch: 9 [6144/60000]\tLoss: 0.412528\n",
      "Train Epoch: 9 [7168/60000]\tLoss: 0.355078\n",
      "Train Epoch: 9 [8192/60000]\tLoss: 0.499474\n",
      "Train Epoch: 9 [9216/60000]\tLoss: 0.440480\n",
      "Train Epoch: 9 [10240/60000]\tLoss: 0.384728\n",
      "Train Epoch: 9 [11264/60000]\tLoss: 0.536549\n",
      "Train Epoch: 9 [12288/60000]\tLoss: 0.534989\n",
      "Train Epoch: 9 [13312/60000]\tLoss: 0.394666\n",
      "Train Epoch: 9 [14336/60000]\tLoss: 0.435361\n",
      "Train Epoch: 9 [15360/60000]\tLoss: 0.449050\n",
      "Train Epoch: 9 [16384/60000]\tLoss: 0.457138\n",
      "Train Epoch: 9 [17408/60000]\tLoss: 0.428490\n",
      "Train Epoch: 9 [18432/60000]\tLoss: 0.479436\n",
      "Train Epoch: 9 [19456/60000]\tLoss: 0.417012\n",
      "Train Epoch: 9 [20480/60000]\tLoss: 0.487194\n",
      "Train Epoch: 9 [21504/60000]\tLoss: 0.413587\n",
      "Train Epoch: 9 [22528/60000]\tLoss: 0.327574\n",
      "Train Epoch: 9 [23552/60000]\tLoss: 0.480535\n",
      "Train Epoch: 9 [24576/60000]\tLoss: 0.670698\n",
      "Train Epoch: 9 [25600/60000]\tLoss: 0.352684\n",
      "Train Epoch: 9 [26624/60000]\tLoss: 0.536956\n",
      "Train Epoch: 9 [27648/60000]\tLoss: 0.660895\n",
      "Train Epoch: 9 [28672/60000]\tLoss: 0.431184\n",
      "Train Epoch: 9 [29696/60000]\tLoss: 0.453355\n",
      "Train Epoch: 9 [30720/60000]\tLoss: 0.493068\n",
      "Train Epoch: 9 [31744/60000]\tLoss: 0.459907\n",
      "Train Epoch: 9 [32768/60000]\tLoss: 0.485521\n",
      "Train Epoch: 9 [33792/60000]\tLoss: 0.374494\n",
      "Train Epoch: 9 [34816/60000]\tLoss: 0.458521\n",
      "Train Epoch: 9 [35840/60000]\tLoss: 0.484626\n",
      "Train Epoch: 9 [36864/60000]\tLoss: 0.486547\n",
      "Train Epoch: 9 [37888/60000]\tLoss: 0.587201\n",
      "Train Epoch: 9 [38912/60000]\tLoss: 0.459765\n",
      "Train Epoch: 9 [39936/60000]\tLoss: 0.377562\n",
      "Train Epoch: 9 [40960/60000]\tLoss: 0.399493\n",
      "Train Epoch: 9 [41984/60000]\tLoss: 0.435362\n",
      "Train Epoch: 9 [43008/60000]\tLoss: 0.383094\n",
      "Train Epoch: 9 [44032/60000]\tLoss: 0.501708\n",
      "Train Epoch: 9 [45056/60000]\tLoss: 0.551931\n",
      "Train Epoch: 9 [46080/60000]\tLoss: 0.592887\n",
      "Train Epoch: 9 [47104/60000]\tLoss: 0.437105\n",
      "Train Epoch: 9 [48128/60000]\tLoss: 0.320313\n",
      "Train Epoch: 9 [49152/60000]\tLoss: 0.440484\n",
      "Train Epoch: 9 [50176/60000]\tLoss: 0.488791\n",
      "Train Epoch: 9 [51200/60000]\tLoss: 0.448329\n",
      "Train Epoch: 9 [52224/60000]\tLoss: 0.548097\n",
      "Train Epoch: 9 [53248/60000]\tLoss: 0.513445\n",
      "Train Epoch: 9 [54272/60000]\tLoss: 0.467013\n",
      "Train Epoch: 9 [55296/60000]\tLoss: 0.274386\n",
      "Train Epoch: 9 [56320/60000]\tLoss: 0.378396\n",
      "Train Epoch: 9 [57344/60000]\tLoss: 0.566657\n",
      "Train Epoch: 9 [58368/60000]\tLoss: 0.724994\n",
      "Train Epoch: 9 [59392/60000]\tLoss: 0.431645\n",
      "\n",
      "Test set: Avg. loss: 0.4298, Accuracy: 8647/10000 (86%)\n",
      "\n",
      "Train Epoch: 10 [0/60000]\tLoss: 0.386166\n",
      "Train Epoch: 10 [1024/60000]\tLoss: 0.538207\n",
      "Train Epoch: 10 [2048/60000]\tLoss: 0.407476\n",
      "Train Epoch: 10 [3072/60000]\tLoss: 0.392989\n",
      "Train Epoch: 10 [4096/60000]\tLoss: 0.504650\n",
      "Train Epoch: 10 [5120/60000]\tLoss: 0.369544\n",
      "Train Epoch: 10 [6144/60000]\tLoss: 0.366457\n",
      "Train Epoch: 10 [7168/60000]\tLoss: 0.437992\n",
      "Train Epoch: 10 [8192/60000]\tLoss: 0.328554\n",
      "Train Epoch: 10 [9216/60000]\tLoss: 0.476728\n",
      "Train Epoch: 10 [10240/60000]\tLoss: 0.426273\n",
      "Train Epoch: 10 [11264/60000]\tLoss: 0.304504\n",
      "Train Epoch: 10 [12288/60000]\tLoss: 0.473306\n",
      "Train Epoch: 10 [13312/60000]\tLoss: 0.522108\n",
      "Train Epoch: 10 [14336/60000]\tLoss: 0.368155\n",
      "Train Epoch: 10 [15360/60000]\tLoss: 0.500524\n",
      "Train Epoch: 10 [16384/60000]\tLoss: 0.375516\n",
      "Train Epoch: 10 [17408/60000]\tLoss: 0.404767\n",
      "Train Epoch: 10 [18432/60000]\tLoss: 0.541735\n",
      "Train Epoch: 10 [19456/60000]\tLoss: 0.380815\n",
      "Train Epoch: 10 [20480/60000]\tLoss: 0.359648\n",
      "Train Epoch: 10 [21504/60000]\tLoss: 0.574451\n",
      "Train Epoch: 10 [22528/60000]\tLoss: 0.377104\n",
      "Train Epoch: 10 [23552/60000]\tLoss: 0.450381\n",
      "Train Epoch: 10 [24576/60000]\tLoss: 0.499486\n",
      "Train Epoch: 10 [25600/60000]\tLoss: 0.413353\n",
      "Train Epoch: 10 [26624/60000]\tLoss: 0.404759\n",
      "Train Epoch: 10 [27648/60000]\tLoss: 0.336503\n",
      "Train Epoch: 10 [28672/60000]\tLoss: 0.445993\n",
      "Train Epoch: 10 [29696/60000]\tLoss: 0.518449\n",
      "Train Epoch: 10 [30720/60000]\tLoss: 0.524929\n",
      "Train Epoch: 10 [31744/60000]\tLoss: 0.504729\n",
      "Train Epoch: 10 [32768/60000]\tLoss: 0.445549\n",
      "Train Epoch: 10 [33792/60000]\tLoss: 0.383818\n",
      "Train Epoch: 10 [34816/60000]\tLoss: 0.363946\n",
      "Train Epoch: 10 [35840/60000]\tLoss: 0.673850\n",
      "Train Epoch: 10 [36864/60000]\tLoss: 0.833049\n",
      "Train Epoch: 10 [37888/60000]\tLoss: 0.334563\n",
      "Train Epoch: 10 [38912/60000]\tLoss: 0.339005\n",
      "Train Epoch: 10 [39936/60000]\tLoss: 0.351114\n",
      "Train Epoch: 10 [40960/60000]\tLoss: 0.419444\n",
      "Train Epoch: 10 [41984/60000]\tLoss: 0.430955\n",
      "Train Epoch: 10 [43008/60000]\tLoss: 0.443621\n",
      "Train Epoch: 10 [44032/60000]\tLoss: 0.383463\n",
      "Train Epoch: 10 [45056/60000]\tLoss: 0.397920\n",
      "Train Epoch: 10 [46080/60000]\tLoss: 0.465962\n",
      "Train Epoch: 10 [47104/60000]\tLoss: 0.513451\n",
      "Train Epoch: 10 [48128/60000]\tLoss: 0.498731\n",
      "Train Epoch: 10 [49152/60000]\tLoss: 0.349466\n",
      "Train Epoch: 10 [50176/60000]\tLoss: 0.584116\n",
      "Train Epoch: 10 [51200/60000]\tLoss: 0.339745\n",
      "Train Epoch: 10 [52224/60000]\tLoss: 0.315113\n",
      "Train Epoch: 10 [53248/60000]\tLoss: 0.410574\n",
      "Train Epoch: 10 [54272/60000]\tLoss: 0.417892\n",
      "Train Epoch: 10 [55296/60000]\tLoss: 0.455905\n",
      "Train Epoch: 10 [56320/60000]\tLoss: 0.445501\n",
      "Train Epoch: 10 [57344/60000]\tLoss: 0.445267\n",
      "Train Epoch: 10 [58368/60000]\tLoss: 0.350945\n",
      "Train Epoch: 10 [59392/60000]\tLoss: 0.378027\n",
      "\n",
      "Test set: Avg. loss: 0.4035, Accuracy: 8704/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get initial performance\n",
    "#test()\n",
    "# Train for three epochs\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162511a1-b80b-43cc-935f-c07b8808f8f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
